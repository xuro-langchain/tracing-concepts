{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading in our environment variables, and setting up an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed tracing lets you associate traces even if they take place on different systems or processes. We'll demonstrate this concept by creating a trace that spans a Server-Client call. \n",
    "\n",
    "The trace will originate in the Client, which will serve as the parent run. We will then trace the Server's execution to appear as a child run under the parent run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by setting up the client, which will originate the trace and send a request to the server. When sending the request, the client will include information about the current trace in its request headers, using the run_tree.to_headers() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import nest_asyncio\n",
    "from typing_extensions import TypedDict\n",
    "from langsmith import Client\n",
    "from langsmith.run_helpers import tracing_context, get_current_run_tree\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Client (Parent) ------------------------------------------------------------ #\n",
    "\n",
    "class ParentState(TypedDict):\n",
    "    input_value: int\n",
    "    output_value: int\n",
    "\n",
    "async def parent_node(state: ParentState) -> ParentState:\n",
    "    print(f\"Parent graph received input: {state['input_value']}\")\n",
    "    headers = {}\n",
    "    async with httpx.AsyncClient(base_url=\"http://localhost:8000\") as client:\n",
    "        run_tree = get_current_run_tree()\n",
    "        headers.update(run_tree.to_headers())\n",
    "        response = await client.post(\"/tracing\", headers=headers, json={\"value\": state[\"input_value\"]})\n",
    "        result = response.json()\n",
    "        print(f\"Child graph returned: {result['value']}\")\n",
    "        return {\"input_value\": state[\"input_value\"], \"output_value\": result[\"value\"]}\n",
    "\n",
    "parent_builder = StateGraph(ParentState)\n",
    "parent_builder.add_node(\"parent_node\", parent_node)\n",
    "parent_builder.add_edge(START, \"parent_node\")\n",
    "parent_builder.add_edge(\"parent_node\", END)\n",
    "parent_graph = parent_builder.compile()\n",
    "\n",
    "async def run_client():\n",
    "    # Run the parent graph with initial input\n",
    "    result = await parent_graph.ainvoke({\"input_value\": 10, \"output_value\": 0})\n",
    "    return result[\"output_value\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create the server, which will receive the trace. It will take in trace metadata sent from the client in the headers, which it will use to set the context of its own trace.\n",
    "\n",
    "This will allow any tracing done in the server to still be associated with the request from the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# server.py\n",
    "import asyncio\n",
    "import threading\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "import uvicorn\n",
    "from dotenv import load_dotenv\n",
    "from fastapi import FastAPI, Request\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langsmith.run_helpers import tracing_context, get_current_run_tree\n",
    "from langsmith import Client\n",
    "\n",
    "\n",
    "# Server (Child) ------------------------------------------------------------ #\n",
    "class ChildState(TypedDict):\n",
    "    value: int\n",
    "\n",
    "async def child_node(state: ChildState):\n",
    "    generation = llm.invoke(\"What is \" + str(state[\"value\"]) + \" 1? Respond with a single number, no extra text.\")\n",
    "    return {\"value\": int(generation.content)}\n",
    "\n",
    "child_builder = StateGraph(ChildState)\n",
    "child_builder.add_node(\"child_node\", child_node)\n",
    "child_builder.add_edge(START, \"child_node\")\n",
    "child_builder.add_edge(\"child_node\", END)\n",
    "child_graph = child_builder.compile()\n",
    "\n",
    "app = FastAPI() \n",
    "\n",
    "@app.post(\"/tracing\")\n",
    "async def tracing(request: Request):\n",
    "    parent_headers = {\n",
    "        \"langsmith-trace\": request.headers.get(\"langsmith-trace\"),\n",
    "        \"baggage\": request.headers.get(\"baggage\"),\n",
    "    }\n",
    "\n",
    "    with tracing_context(parent=parent_headers):\n",
    "        data = await request.json()\n",
    "        result = await child_graph.ainvoke({\"value\": data[\"value\"]})\n",
    "        return result\n",
    "\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, loop=\"asyncio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Our Distributed System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put this all into action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [64819]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent graph received input: 10\n",
      "INFO:     127.0.0.1:56716 - \"POST /tracing HTTP/1.1\" 200 OK\n",
      "Child graph returned: 11\n",
      "Server replied: 11\n"
     ]
    }
   ],
   "source": [
    "# ---------- Main ---------- #\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def main():\n",
    "    thread = threading.Thread(target=run_server, daemon=True)\n",
    "    thread.start()\n",
    "    await asyncio.sleep(1)\n",
    "\n",
    "    result = await run_client()\n",
    "    print(\"Server replied:\", result)\n",
    "    await asyncio.sleep(1)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Now let's view the trace in LangSmith!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
