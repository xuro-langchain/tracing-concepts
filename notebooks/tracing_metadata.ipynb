{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing Metadata & Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in our environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, you can use the trace context manager to add metadata to your traces. This can be useful for querying, grouping, and aggregating your trace information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langsmith import traceable, trace\n",
    "import langsmith as ls\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "MODEL_PROVIDER = \"openai\"\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "openai_client = OpenAI()\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable(\n",
    "  run_type=\"chain\",\n",
    "  tags=[\"retriever-1.0\"],\n",
    "  metadata={\"datasource\": \"docs.smith.langchain.com\"}\n",
    ")\n",
    "def retrieve_documents(question: str):\n",
    "    documents = retriever.invoke(question)\n",
    "    quality_docs = [doc for doc in documents if len(doc.page_content) > 20]\n",
    "    rt = ls.get_current_run_tree()\n",
    "    if len(quality_docs) > 3:\n",
    "        rt.metadata[\"data-availability\"] = \"high\"\n",
    "    else:\n",
    "        rt.metadata[\"data-availability\"] = \"low\"\n",
    "    return documents\n",
    "\n",
    "\n",
    "@traceable\n",
    "def generate_response(question: str, documents):\n",
    "    # NOTE: Our documents came in as a list of objects, but we just want to log a string\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    response = call_openai(messages)\n",
    "    return response\n",
    "\n",
    "\"\"\"\n",
    "call_openai\n",
    "- Returns the chat completion output from OpenAI\n",
    "\"\"\"\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_openai(\n",
    "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    ") -> str:\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable\n",
    "def metadata_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To add metadata to your trace, you can follow the instructions provided in the LangSmith documentation on adding metadata to your traces. This typically involves attaching key-value pairs that store additional information about the run. For detailed steps, refer to the \"Add metadata and tags to traces\" section in the LangSmith UI.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I add metadata to my trace?\"\n",
    "ai_answer = metadata_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nocturnal animals typically eat a variety of foods depending on their species, including insects, small mammals, fruits, and plants. Their diet can vary widely, but many are predators or scavengers. Some nocturnal animals, like owls, primarily hunt for small animals, while others, like bats, may feed on insects or fruit.\n"
     ]
    }
   ],
   "source": [
    "question = \"Nocturnal animals eat what?\"\n",
    "ai_answer = metadata_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've added metadata to your traces, you can then utilize this metadata to help you query and organize your traces. This can be accomplished through LangSmith's SDK or API. We'll initialize a LangSmith client to utilize the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "import os\n",
    "\n",
    "client = Client()\n",
    "\n",
    "project = os.getenv(\"LANGSMITH_PROJECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also define a helper function for us to print information retrieved about our traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trunc_runs(runs):\n",
    "    try:\n",
    "        for _ in range(3):\n",
    "            run = next(runs)\n",
    "            print(f\"Run ID: {run.id}\")\n",
    "            print(f\"Name: {run.name}\")\n",
    "            print(f\"Run Type: {run.run_type}\")\n",
    "            print(f\"Start Time: {run.start_time}\")\n",
    "            print(f\"Inputs: {str(run.inputs)[:50]}...\")\n",
    "            print(f\"Outputs: {str(run.outputs)[:50]}...\")\n",
    "            print(\"-\" * 40)\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: 76a7cf38-117e-4205-b1dc-227618d57a5a\n",
      "Name: call_openai\n",
      "Run Type: llm\n",
      "Start Time: 2025-07-22 13:55:29.401532\n",
      "Inputs: {'messages': [{'role': 'system', 'content': \"You a...\n",
      "Outputs: {'id': 'chatcmpl-Bw7knawr5S7HhfSiuNCMRKGoDx5gw', '...\n",
      "----------------------------------------\n",
      "Run ID: be302ff6-cead-4c51-844d-da7e28b60720\n",
      "Name: OpenAI Call\n",
      "Run Type: llm\n",
      "Start Time: 2025-07-22 13:55:02.580592\n",
      "Inputs: {'messages': [{'role': 'system', 'content': \"You a...\n",
      "Outputs: {'openai_response': {'id': 'chatcmpl-Bw7kM9rqeFoqA...\n",
      "----------------------------------------\n",
      "Run ID: 4c5c5d67-ed0d-4a22-b309-4a3c4a44601b\n",
      "Name: ChatOpenAI\n",
      "Run Type: llm\n",
      "Start Time: 2025-07-22 13:54:44.646290\n",
      "Inputs: {'messages': [[{'lc': 1, 'type': 'constructor', 'i...\n",
      "Outputs: {'generations': [[{'text': 'To set up tracing with...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "todays_successful_llm_runs = client.list_runs(\n",
    "  project_name=project,\n",
    "  start_time=datetime.now() - timedelta(days=1),\n",
    "  run_type=\"llm\",\n",
    "  error=False\n",
    ")\n",
    "\n",
    "print_trunc_runs(todays_successful_llm_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use LangSmith's filter query language to check for which runs have a high data-availability based on our metadata tags. This should only return retrieve_documents calls because we set the metadata on that run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: c78932a2-4ce4-450a-b18f-55f93f49351d\n",
      "Name: retrieve_documents\n",
      "Run Type: chain\n",
      "Start Time: 2025-07-22 13:55:28.677991\n",
      "Inputs: {'question': 'How do I add metadata to my trace?'}...\n",
      "Outputs: {'output': [{'metadata': {'id': '2508a010-9446-461...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "high_available_runs = client.list_runs(\n",
    "  project_name=project,\n",
    "  start_time=datetime.now() - timedelta(days=1),\n",
    "  filter=\"and(eq(metadata_key, 'data-availability'), eq(metadata_value, 'high'))\",\n",
    "  error=False,\n",
    ")\n",
    "\n",
    "print_trunc_runs(high_available_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter based on tags to check which version we're using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: 6c89ee27-ef01-44d3-98c6-34895636e283\n",
      "Name: VectorStoreRetriever\n",
      "Run Type: retriever\n",
      "Start Time: 2025-07-22 13:55:31.492121\n",
      "Inputs: {'query': 'Nocturnal animals eat what?'}...\n",
      "Outputs: {'documents': [{'metadata': {'id': '20449fce-1e7d-...\n",
      "----------------------------------------\n",
      "Run ID: 28543911-80be-4da0-8127-ece616d8ba59\n",
      "Name: retrieve_documents\n",
      "Run Type: chain\n",
      "Start Time: 2025-07-22 13:55:31.491690\n",
      "Inputs: {'question': 'Nocturnal animals eat what?'}...\n",
      "Outputs: {'output': [{'metadata': {'id': '20449fce-1e7d-4b4...\n",
      "----------------------------------------\n",
      "Run ID: 7f9975a1-8683-4d16-b587-be7497e08041\n",
      "Name: VectorStoreRetriever\n",
      "Run Type: retriever\n",
      "Start Time: 2025-07-22 13:55:28.680695\n",
      "Inputs: {'query': 'How do I add metadata to my trace?'}...\n",
      "Outputs: {'documents': [{'metadata': {'id': '2508a010-9446-...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "retriever_v1_runs = client.list_runs(\n",
    "  project_name=project,\n",
    "  start_time=datetime.now() - timedelta(days=1),\n",
    "  filter=\"has(tags, 'retriever-1.0')\",\n",
    "  error=False,\n",
    ")\n",
    "\n",
    "print_trunc_runs(retriever_v1_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily combine complex criteria using filter queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: 6c89ee27-ef01-44d3-98c6-34895636e283\n",
      "Name: VectorStoreRetriever\n",
      "Run Type: retriever\n",
      "Start Time: 2025-07-22 13:55:31.492121\n",
      "Inputs: {'query': 'Nocturnal animals eat what?'}...\n",
      "Outputs: {'documents': [{'metadata': {'id': '20449fce-1e7d-...\n",
      "----------------------------------------\n",
      "Run ID: 28543911-80be-4da0-8127-ece616d8ba59\n",
      "Name: retrieve_documents\n",
      "Run Type: chain\n",
      "Start Time: 2025-07-22 13:55:31.491690\n",
      "Inputs: {'question': 'Nocturnal animals eat what?'}...\n",
      "Outputs: {'output': [{'metadata': {'id': '20449fce-1e7d-4b4...\n",
      "----------------------------------------\n",
      "Run ID: 1761b20c-c2f7-44b7-b84f-7f18b0215e6f\n",
      "Name: VectorStoreRetriever\n",
      "Run Type: retriever\n",
      "Start Time: 2025-07-20 03:13:29.654445\n",
      "Inputs: {'query': 'Nocturnal animals eat what?'}...\n",
      "Outputs: {'documents': [{'metadata': {'id': '6e3d78fa-b4b6-...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "slow_retriever_runs = client.list_runs(\n",
    "  project_name=project,\n",
    "  filter=\"and(has(tags, 'retriever-1.0'), gt(latency, 0.2), search('Nocturnal'))\",\n",
    "  error=False,\n",
    ")\n",
    "\n",
    "print_trunc_runs(slow_retriever_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complex queries, we can use tree and trace filters. \n",
    "tree filters are conditions that are fulfilled if they're met by any run within the trace tree (children, siblings, etc.).\n",
    "trace filters are conditions that are fulfilled if they're met by the root run of the trace tree (parent).\n",
    "\n",
    "For example, let's query all runs named \"call_openai\" whose root run has a latency less than 2 seconds and whose trace contains a run with \"high\" \"data-availability\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: f518f8cc-5a6d-4ac6-a80f-8c5dfc3ef6f9\n",
      "Name: call_openai\n",
      "Run Type: llm\n",
      "Start Time: 2025-07-20 03:13:27.093930\n",
      "Inputs: {'messages': [{'role': 'system', 'content': \"You a...\n",
      "Outputs: {'id': 'chatcmpl-BvEmNLHwOGn74o3uHh04E4m2X3dhg', '...\n",
      "----------------------------------------\n",
      "Run ID: 6724b322-df70-4646-b64c-76d2e2904195\n",
      "Name: call_openai\n",
      "Run Type: chain\n",
      "Start Time: 2025-07-20 01:22:12.063047\n",
      "Inputs: {'messages': [{'role': 'system', 'content': \"You a...\n",
      "Outputs: {'id': 'chatcmpl-BvD2iUuqR4OKgi0klbjgAPOqI9AMv', '...\n",
      "----------------------------------------\n",
      "Run ID: 6a018638-32f8-451a-a2d6-c749acea02cb\n",
      "Name: call_openai\n",
      "Run Type: chain\n",
      "Start Time: 2025-07-20 01:21:49.257544\n",
      "Inputs: {'messages': [{'role': 'system', 'content': \"You a...\n",
      "Outputs: {'id': 'chatcmpl-BvD2LotIZZpslCNCAOavMQRCmLbCi', '...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fast_high_availability_llm_calls = client.list_runs(\n",
    "    project_name=project,\n",
    "    filter='eq(name, \"call_openai\")',\n",
    "    trace_filter='lt(latency, 2)',\n",
    "    tree_filter=\"and(eq(metadata_key, 'data-availability'), eq(metadata_value, 'high'))\"\n",
    ")\n",
    "print_trunc_runs(fast_high_availability_llm_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
